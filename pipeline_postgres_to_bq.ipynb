{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T21:00:31.006409Z",
     "start_time": "2024-12-19T21:00:23.486650Z"
    }
   },
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import os\n",
    "import psycopg2 as pg\n",
    "\n",
    "SERVICE_ACCOUNT = \"curso-apache-beam-gcp.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = SERVICE_ACCOUNT\n",
    "\n",
    "CLOUD_STORAGE = \"gs://curso-apache-beam-gcp-n\"\n",
    "CLOUD_STORAGE_TEMP = f\"{CLOUD_STORAGE}/temp\"\n",
    "CLOUD_STORAGE_TEMPLATE_FILE = f\"{CLOUD_STORAGE}/template/curso-apache-beam-postgres-bq-local-teste\"\n",
    "\n",
    "BIG_QUERY_TABLE = f\"curso-apache-beam-gcp:cursoapachebeamgcpdataset.users\"\n",
    "\n",
    "pipelines_options = PipelineOptions.from_dictionary({\n",
    "    'project': 'curso-apache-beam-gcp',\n",
    "    'runner': 'DataflowRunner',\n",
    "    'region': 'us',\n",
    "    'staging_location': CLOUD_STORAGE_TEMP,\n",
    "    'temp_location': CLOUD_STORAGE_TEMP,\n",
    "    'template_location': CLOUD_STORAGE_TEMPLATE_FILE,\n",
    "    'save_main_session': True,\n",
    "})\n",
    "\n",
    "class ReadUsersFromPostgres(beam.DoFn):\n",
    "\n",
    "    def __init__(self, *unused_args, **unused_kwargs):\n",
    "        super().__init__(*unused_args, **unused_kwargs)\n",
    "        self.db_config = {\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": 5432,\n",
    "            \"database\": \"postgres\",\n",
    "            \"user\": \"postgres\",\n",
    "        }\n",
    "\n",
    "    def setup(self):\n",
    "        # Establish the database connection\n",
    "        self.conn = pg.connect(**self.db_config)\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def process(self, element):\n",
    "        # Fetch data from the database\n",
    "        self.cursor.execute(\"SELECT * FROM users\")\n",
    "        for row in self.cursor.fetchall():\n",
    "            yield row\n",
    "\n",
    "    def teardown(self):\n",
    "        # Close the database connection\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "\n",
    "\n",
    "class FormatUsersToBq(beam.DoFn):\n",
    "\n",
    "    def __init__(self, *unused_args, **unused_kwargs):\n",
    "        super().__init__(*unused_args, **unused_kwargs)\n",
    "\n",
    "    def process(self, element):\n",
    "        name, email, age = element\n",
    "\n",
    "        # Validate and clean data\n",
    "        if not isinstance(name, str) or not name.strip():\n",
    "            name = \"Unknown\"\n",
    "\n",
    "        if not isinstance(email, str) or \"@\" not in email or \".\" not in email:\n",
    "            email = \"invalid@example.com\"\n",
    "\n",
    "        if not isinstance(age, int) or age <= 0:\n",
    "            age = None  # Use NULL in BigQuery for invalid ages\n",
    "\n",
    "        # Yield formatted dictionary\n",
    "        yield {\n",
    "            \"Name\": name,\n",
    "            \"Email\": email,\n",
    "            \"Age\": age\n",
    "        }\n",
    "\n",
    "\n",
    "with beam.Pipeline(\n",
    "        options=pipelines_options\n",
    ") as p:\n",
    "    postgres_rows = (\n",
    "            p\n",
    "            | \"Create Input\" >> beam.Create([None])\n",
    "            | \"Lendo Bando de Dados\" >> beam.ParDo(ReadUsersFromPostgres())\n",
    "            | \"Formatando dados para o Big Query\" >> beam.ParDo(FormatUsersToBq())\n",
    "            | 'Enviando para o Big Query' >> beam.io.WriteToBigQuery(\n",
    "        table=BIG_QUERY_TABLE,\n",
    "        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "        custom_gcs_temp_location=CLOUD_STORAGE_TEMP)\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://curso-apache-beam-gcp-n/temp for cache location.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T21:30:55.723635Z",
     "start_time": "2024-12-19T21:30:54.186843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google.cloud.secretmanager import GetSecretRequest\n",
    "\n",
    "def get_secret(project_id: str, secret_id: str) -> GetSecretRequest:\n",
    "    \"\"\"\n",
    "    Get information about the given secret. This only returns metadata about\n",
    "    the secret container, not any secret material.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the Secret Manager client library.\n",
    "    from google.cloud import secretmanager\n",
    "\n",
    "    # Create the Secret Manager client.\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "    # Build the resource name of the secret.\n",
    "    name = client.secret_path(project_id, secret_id)\n",
    "\n",
    "    # Get the secret.\n",
    "    response = client.get_secret(request={\"name\": name})\n",
    "\n",
    "    # Get the replication policy.\n",
    "    if \"automatic\" in response.replication:\n",
    "        replication = \"AUTOMATIC\"\n",
    "    elif \"user_managed\" in response.replication:\n",
    "        replication = \"MANAGED\"\n",
    "    else:\n",
    "        raise Exception(f\"Unknown replication {response.replication}\")\n",
    "\n",
    "    # Print data about the secret.\n",
    "    print(f\"Got secret {response.name} with replication policy {replication}\")\n",
    "\n",
    "get_secret(\"curso-apache-beam-gcp\", \"coiso\")"
   ],
   "id": "93a7e674e644ad70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got secret projects/223295794346/secrets/coiso with replication policy AUTOMATIC\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
