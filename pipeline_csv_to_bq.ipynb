{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T19:42:15.798817Z",
     "start_time": "2024-12-19T19:42:14.974950Z"
    }
   },
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from formatter_classes import SplitByCommasDoFn, FilterWithDelaysDoFn, CreateKeyValueDoFn, FormatForBigQueryDoFn\n",
    "import os\n",
    "\n",
    "SERVICE_ACCOUNT = \"curso-apache-beam-gcp.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = SERVICE_ACCOUNT\n",
    "\n",
    "CLOUD_STORAGE = \"gs://curso-apache-beam-gcp-n\"\n",
    "CLOUD_STORAGE_INPUT = \"gs://curso-apache-beam-gcp-n/inputs/voos_sample.csv\"\n",
    "CLOUD_STORAGE_TEMP = f\"{CLOUD_STORAGE}/temp\"\n",
    "CLOUD_STORAGE_TEMPLATE_FILE = f\"{CLOUD_STORAGE}/template/curso-apache-beam-bckt-bq-local-teste\"\n",
    "\n",
    "BIG_QUERY_TABLE = f\"curso-apache-beam-gcp:cursoapachebeamgcpdataset.cursoapachebeamgcpdataset-flights\"\n",
    "\n",
    "pipelines_options = PipelineOptions.from_dictionary({\n",
    "    'project': 'curso-apache-beam-gcp',\n",
    "    'runner': 'DataflowRunner',\n",
    "    'region': 'us',\n",
    "    'staging_location': CLOUD_STORAGE_TEMP,\n",
    "    'temp_location': CLOUD_STORAGE_TEMP,\n",
    "    'template_location': CLOUD_STORAGE_TEMPLATE_FILE,\n",
    "    'save_main_session': True\n",
    "})\n",
    "\n",
    "p = beam.Pipeline(options=pipelines_options)\n",
    "\n",
    "def process_data(pipeline: beam.Pipeline, type_of_pipeline: str):\n",
    "    return (\n",
    "            pipeline\n",
    "            | f\"Importar Dados ({type_of_pipeline})\" >> beam.io.ReadFromText(CLOUD_STORAGE_INPUT, skip_header_lines=1)\n",
    "            | f\"Separar por Virgulas ({type_of_pipeline})\" >> beam.ParDo(SplitByCommasDoFn())\n",
    "            | f\"Voos sem atraso ({type_of_pipeline})\" >> beam.Filter(FilterWithDelaysDoFn())\n",
    "            | f\"Criar par ({type_of_pipeline})\" >> beam.Map(CreateKeyValueDoFn())\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GROUP BY + COUNT\n",
    "contagem_atrasos = (\n",
    "        process_data(p, \"Contagem Atrasos\")\n",
    "        | \"Contar por key\" >> beam.combiners.Count.PerKey()\n",
    ")"
   ],
   "id": "e333cb6c4154ba6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GROUP BY + SUM\n",
    "soma_atrasos = (\n",
    "        process_data(p, \"Soma Atrasos\")\n",
    "        | \"Somar por key\" >> beam.CombinePerKey(sum)\n",
    ")"
   ],
   "id": "77663feef49fb13a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Formatting and sending to BQ\n",
    "\n",
    "tabela_atrasos = (\n",
    "        {'contagem_atrasos': contagem_atrasos, 'soma_atrasos': soma_atrasos}\n",
    "        | 'Agrupando por chave' >> beam.CoGroupByKey()\n",
    "        | 'Transformar para formato BigQuery' >> beam.ParDo(FormatForBigQueryDoFn())\n",
    "        | 'Enviando para o Big Query' >> beam.io.WriteToBigQuery(\n",
    "    table=BIG_QUERY_TABLE,\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "    custom_gcs_temp_location=CLOUD_STORAGE_TEMP)\n",
    ")"
   ],
   "id": "426d61cbcb957094"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Running pipeline\n",
    "p.run()"
   ],
   "id": "736b540139f5c3ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
